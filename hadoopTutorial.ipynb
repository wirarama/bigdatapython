{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]\r\n",
      "  CLASSNAME            run the class named CLASSNAME\r\n",
      " or\r\n",
      "  where COMMAND is one of:\r\n",
      "  fs                   run a generic filesystem user client\r\n",
      "  version              print the version\r\n",
      "  jar <jar>            run a jar file\r\n",
      "                       note: please use \"yarn jar\" to launch\r\n",
      "                             YARN applications, not this command.\r\n",
      "  checknative [-a|-h]  check native hadoop and compression libraries availability\r\n",
      "  distcp <srcurl> <desturl> copy file or directories recursively\r\n",
      "  archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive\r\n",
      "  classpath            prints the class path needed to get the\r\n",
      "  credential           interact with credential providers\r\n",
      "                       Hadoop jar and the required libraries\r\n",
      "  daemonlog            get/set the log level for each daemon\r\n",
      "  trace                view and modify Hadoop tracing settings\r\n",
      "\r\n",
      "Most commands print help when invoked w/o parameters.\r\n"
     ]
    }
   ],
   "source": [
    "!/home/wirarama/hadoop/bin/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!xed ~/hadoop/etc/hadoop/core-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<configuration>\n",
    "    <property>\n",
    "        <name>fs.defaultFS</name>\n",
    "        <value>hdfs://localhost:9000</value>\n",
    "    </property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!xed ~/hadoop/etc/hadoop/hdfs-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<configuration>\n",
    "    <property>\n",
    "        <name>dfs.replication</name>\n",
    "        <value>1</value>\n",
    "    </property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/10/01 09:28:31 INFO namenode.NameNode: STARTUP_MSG: \r\n",
      "/************************************************************\r\n",
      "STARTUP_MSG: Starting NameNode\r\n",
      "STARTUP_MSG:   host = wirarama-K45VD/127.0.1.1\r\n",
      "STARTUP_MSG:   args = [-format]\r\n",
      "STARTUP_MSG:   version = 2.7.7\r\n",
      "STARTUP_MSG:   classpath = /home/wirarama/hadoop/etc/hadoop:/home/wirarama/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/wirarama/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/wirarama/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/wirarama/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/wirarama/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/wirarama/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/wirarama/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/wirarama/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/wirarama/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/wirarama/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/wirarama/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/wirarama/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/wirarama/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/wirarama/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/wirarama/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/wirarama/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/wirarama/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/wirarama/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/wirarama/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/wirarama/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/wirarama/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/wirarama/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/wirarama/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/wirarama/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/wirarama/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/wirarama/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/wirarama/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/wirarama/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/wirarama/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/wirarama/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/wirarama/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/wirarama/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/wirarama/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/wirarama/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/wirarama/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/wirarama/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/wirarama/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/wirarama/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/wirarama/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/wirarama/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/wirarama/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/wirarama/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/wirarama/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/wirarama/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/wirarama/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/wirarama/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/wirarama/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/wirarama/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/wirarama/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/wirarama/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/wirarama/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/wirarama/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/wirarama/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/wirarama/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/wirarama/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/wirarama/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/wirarama/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/wirarama/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/wirarama/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/wirarama/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/wirarama/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/wirarama/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/wirarama/hadoop/share/hadoop/common/hadoop-common-2.7.7-tests.jar:/home/wirarama/hadoop/share/hadoop/common/hadoop-nfs-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/common/hadoop-common-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/hdfs:/home/wirarama/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/wirarama/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/wirarama/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/wirarama/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/wirarama/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/wirarama/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/wirarama/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/wirarama/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/wirarama/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/wirarama/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/wirarama/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/wirarama/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/wirarama/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/wirarama/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/wirarama/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/wirarama/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/wirarama/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/wirarama/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/wirarama/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/wirarama/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/wirarama/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/wirarama/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/wirarama/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/wirarama/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/wirarama/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/wirarama/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.7-tests.jar:/home/wirarama/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/wirarama/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/wirarama/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.7.jar:/home/wirarama/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7-tests.jar:/home/wirarama/hadoop/contrib/capacity-scheduler/*.jar\r\n",
      "STARTUP_MSG:   build = Unknown -r c1aad84bd27cd79c3d1a7dd58202a8c3ee1ed3ac; compiled by 'stevel' on 2018-07-18T22:47Z\r\n",
      "STARTUP_MSG:   java = 1.8.0_211\r\n",
      "************************************************************/\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/10/01 09:28:32 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
      "19/10/01 09:28:32 INFO namenode.NameNode: createNameNode [-format]\n",
      "Formatting using clusterid: CID-6a11d982-fdea-4ba9-a367-4cfe7b064b70\n",
      "19/10/01 09:28:34 INFO namenode.FSNamesystem: No KeyProvider found.\n",
      "19/10/01 09:28:34 INFO namenode.FSNamesystem: fsLock is fair: true\n",
      "19/10/01 09:28:34 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
      "19/10/01 09:28:34 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000\n",
      "19/10/01 09:28:34 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
      "19/10/01 09:28:34 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
      "19/10/01 09:28:34 INFO blockmanagement.BlockManager: The block deletion will start around 2019 Oct 01 09:28:34\n",
      "19/10/01 09:28:34 INFO util.GSet: Computing capacity for map BlocksMap\n",
      "19/10/01 09:28:34 INFO util.GSet: VM type       = 64-bit\n",
      "19/10/01 09:28:34 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB\n",
      "19/10/01 09:28:34 INFO util.GSet: capacity      = 2^21 = 2097152 entries\n",
      "19/10/01 09:28:34 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false\n",
      "19/10/01 09:28:34 INFO blockmanagement.BlockManager: defaultReplication         = 1\n",
      "19/10/01 09:28:34 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
      "19/10/01 09:28:34 INFO blockmanagement.BlockManager: minReplication             = 1\n",
      "19/10/01 09:28:34 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
      "19/10/01 09:28:34 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000\n",
      "19/10/01 09:28:34 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
      "19/10/01 09:28:34 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
      "19/10/01 09:28:34 INFO namenode.FSNamesystem: fsOwner             = wirarama (auth:SIMPLE)\n",
      "19/10/01 09:28:34 INFO namenode.FSNamesystem: supergroup          = supergroup\n",
      "19/10/01 09:28:34 INFO namenode.FSNamesystem: isPermissionEnabled = true\n",
      "19/10/01 09:28:34 INFO namenode.FSNamesystem: HA Enabled: false\n",
      "19/10/01 09:28:34 INFO namenode.FSNamesystem: Append Enabled: true\n",
      "19/10/01 09:28:36 INFO util.GSet: Computing capacity for map INodeMap\n",
      "19/10/01 09:28:36 INFO util.GSet: VM type       = 64-bit\n",
      "19/10/01 09:28:36 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB\n",
      "19/10/01 09:28:36 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
      "19/10/01 09:28:36 INFO namenode.FSDirectory: ACLs enabled? false\n",
      "19/10/01 09:28:36 INFO namenode.FSDirectory: XAttrs enabled? true\n",
      "19/10/01 09:28:36 INFO namenode.FSDirectory: Maximum size of an xattr: 16384\n",
      "19/10/01 09:28:36 INFO namenode.NameNode: Caching file names occuring more than 10 times\n",
      "19/10/01 09:28:36 INFO util.GSet: Computing capacity for map cachedBlocks\n",
      "19/10/01 09:28:36 INFO util.GSet: VM type       = 64-bit\n",
      "19/10/01 09:28:36 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB\n",
      "19/10/01 09:28:36 INFO util.GSet: capacity      = 2^18 = 262144 entries\n",
      "19/10/01 09:28:36 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n",
      "19/10/01 09:28:36 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0\n",
      "19/10/01 09:28:36 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000\n",
      "19/10/01 09:28:36 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
      "19/10/01 09:28:36 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
      "19/10/01 09:28:36 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
      "19/10/01 09:28:36 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
      "19/10/01 09:28:36 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
      "19/10/01 09:28:36 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
      "19/10/01 09:28:36 INFO util.GSet: VM type       = 64-bit\n",
      "19/10/01 09:28:36 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB\n",
      "19/10/01 09:28:36 INFO util.GSet: capacity      = 2^15 = 32768 entries\n",
      "19/10/01 09:28:36 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1256892185-127.0.1.1-1569893316609\n",
      "19/10/01 09:28:36 INFO common.Storage: Storage directory /tmp/hadoop-wirarama/dfs/name has been successfully formatted.\n",
      "19/10/01 09:28:37 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-wirarama/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n",
      "19/10/01 09:28:37 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-wirarama/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 325 bytes saved in 0 seconds.\n",
      "19/10/01 09:28:37 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
      "19/10/01 09:28:37 INFO util.ExitUtil: Exiting with status 0\n",
      "19/10/01 09:28:37 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
      "/************************************************************\n",
      "SHUTDOWN_MSG: Shutting down NameNode at wirarama-K45VD/127.0.1.1\n",
      "************************************************************/\n"
     ]
    }
   ],
   "source": [
    "!~/hadoop/bin/hdfs namenode -format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /home/wirarama/hadoop/logs/hadoop-wirarama-namenode-wirarama-K45VD.out\n",
      "localhost: starting datanode, logging to /home/wirarama/hadoop/logs/hadoop-wirarama-datanode-wirarama-K45VD.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /home/wirarama/hadoop/logs/hadoop-wirarama-secondarynamenode-wirarama-K45VD.out\n"
     ]
    }
   ],
   "source": [
    "!~/hadoop/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:50070/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/hadoop/bin/hdfs dfs -mkdir /user\n",
    "!~/hadoop/bin/hdfs dfs -mkdir /user/wirarama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/hadoop/bin/hdfs dfs -put ~/hadoop/etc/hadoop input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/hadoop/bin/hadoop jar ~/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar grep input output 'dfs[a-z.]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/hadoop/bin/hdfs dfs -get output output\n",
    "!cat output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/hadoop/bin/hdfs dfs -cat output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n"
     ]
    }
   ],
   "source": [
    "!~/hadoop/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!xed ~/hadoop/etc/hadoop/mapred-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<?xml version=\"1.0\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>mapreduce.framework.name</name>\n",
    "        <value>yarn</value>\n",
    "    </property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!xed ~/hadoop/etc/hadoop/yarn-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<configuration>\n",
    "    <property>\n",
    "        <name>yarn.nodemanager.aux-services</name>\n",
    "        <value>mapreduce_shuffle</value>\n",
    "    </property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/hadoop/sbin/start-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:8088/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ~/hadoop/sbin/stop-yarn.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/hadoop/bin/hdfs dfs -rm -R python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/hadoop/bin/hdfs dfs -rm -R loremipsum-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/hadoop/bin/hdfs dfs -put -f ~/hadoop/python python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\r\n",
      "drwxr-xr-x   - wirarama supergroup          0 2019-09-30 10:56 python/loremipsum\r\n",
      "-rw-r--r--   1 wirarama supergroup        166 2019-09-30 10:56 python/mapper.py\r\n",
      "-rw-r--r--   1 wirarama supergroup        570 2019-09-30 10:56 python/reducer.py\r\n",
      "-rw-r--r--   1 wirarama supergroup        611 2019-09-30 10:56 python/test.txt\r\n",
      "-rw-r--r--   1 wirarama supergroup        591 2019-09-30 10:56 python/testhadoop.txt\r\n"
     ]
    }
   ],
   "source": [
    "!~/hadoop/bin/hdfs dfs -ls python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/09/30 11:03:09 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "19/09/30 11:03:09 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "19/09/30 11:03:09 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "19/09/30 11:03:10 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/09/30 11:03:10 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "19/09/30 11:03:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local545581902_0001\n",
      "19/09/30 11:03:10 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "19/09/30 11:03:10 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "19/09/30 11:03:10 INFO mapreduce.Job: Running job: job_local545581902_0001\n",
      "19/09/30 11:03:10 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "19/09/30 11:03:10 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/09/30 11:03:11 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "19/09/30 11:03:11 INFO mapred.LocalJobRunner: Starting task: attempt_local545581902_0001_m_000000_0\n",
      "19/09/30 11:03:11 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/09/30 11:03:11 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "19/09/30 11:03:11 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/wirarama/python/loremipsum/loremipsum.txt:0+1391\n",
      "19/09/30 11:03:11 INFO mapred.MapTask: numReduceTasks: 1\n",
      "19/09/30 11:03:11 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "19/09/30 11:03:11 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "19/09/30 11:03:11 INFO mapred.MapTask: soft limit at 83886080\n",
      "19/09/30 11:03:11 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "19/09/30 11:03:11 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "19/09/30 11:03:11 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "19/09/30 11:03:11 INFO streaming.PipeMapRed: PipeMapRed exec [/home/wirarama/hadoop/python/mapper.py]\n",
      "19/09/30 11:03:11 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "19/09/30 11:03:11 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "19/09/30 11:03:11 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "19/09/30 11:03:11 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "19/09/30 11:03:11 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "19/09/30 11:03:11 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "19/09/30 11:03:11 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "19/09/30 11:03:11 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "19/09/30 11:03:11 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "19/09/30 11:03:11 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "19/09/30 11:03:11 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "19/09/30 11:03:11 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "19/09/30 11:03:11 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/09/30 11:03:11 INFO streaming.PipeMapRed: Records R/W=3/1\n",
      "19/09/30 11:03:11 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "19/09/30 11:03:11 INFO streaming.PipeMapRed: mapRedFinished\n",
      "19/09/30 11:03:11 INFO mapred.LocalJobRunner: \n",
      "19/09/30 11:03:11 INFO mapred.MapTask: Starting flush of map output\n",
      "19/09/30 11:03:11 INFO mapred.MapTask: Spilling map output\n",
      "19/09/30 11:03:11 INFO mapred.MapTask: bufstart = 0; bufend = 1796; bufvoid = 104857600\n",
      "19/09/30 11:03:11 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213588(104854352); length = 809/6553600\n",
      "19/09/30 11:03:11 INFO mapred.MapTask: Finished spill 0\n",
      "19/09/30 11:03:11 INFO mapred.Task: Task:attempt_local545581902_0001_m_000000_0 is done. And is in the process of committing\n",
      "19/09/30 11:03:11 INFO mapred.LocalJobRunner: Records R/W=3/1\n",
      "19/09/30 11:03:11 INFO mapred.Task: Task 'attempt_local545581902_0001_m_000000_0' done.\n",
      "19/09/30 11:03:11 INFO mapred.Task: Final Counters for attempt_local545581902_0001_m_000000_0: Counters: 22\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=129624\n",
      "\t\tFILE: Number of bytes written=433966\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1391\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=6\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=1\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3\n",
      "\t\tMap output records=203\n",
      "\t\tMap output bytes=1796\n",
      "\t\tMap output materialized bytes=2208\n",
      "\t\tInput split bytes=120\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=203\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=242221056\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1391\n",
      "19/09/30 11:03:11 INFO mapred.LocalJobRunner: Finishing task: attempt_local545581902_0001_m_000000_0\n",
      "19/09/30 11:03:11 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "19/09/30 11:03:11 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "19/09/30 11:03:11 INFO mapred.LocalJobRunner: Starting task: attempt_local545581902_0001_r_000000_0\n",
      "19/09/30 11:03:11 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/09/30 11:03:11 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "19/09/30 11:03:11 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5580386a\n",
      "19/09/30 11:03:11 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "19/09/30 11:03:11 INFO reduce.EventFetcher: attempt_local545581902_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "19/09/30 11:03:11 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local545581902_0001_m_000000_0 decomp: 2204 len: 2208 to MEMORY\n",
      "19/09/30 11:03:11 INFO reduce.InMemoryMapOutput: Read 2204 bytes from map-output for attempt_local545581902_0001_m_000000_0\n",
      "19/09/30 11:03:11 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2204, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2204\n",
      "19/09/30 11:03:11 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "19/09/30 11:03:11 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/09/30 11:03:11 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "19/09/30 11:03:11 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/09/30 11:03:11 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2194 bytes\n",
      "19/09/30 11:03:11 INFO mapreduce.Job: Job job_local545581902_0001 running in uber mode : false\n",
      "19/09/30 11:03:11 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/09/30 11:03:11 INFO reduce.MergeManagerImpl: Merged 1 segments, 2204 bytes to disk to satisfy reduce memory limit\n",
      "19/09/30 11:03:11 INFO reduce.MergeManagerImpl: Merging 1 files, 2208 bytes from disk\n",
      "19/09/30 11:03:11 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "19/09/30 11:03:11 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/09/30 11:03:11 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2194 bytes\n",
      "19/09/30 11:03:11 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/09/30 11:03:11 INFO streaming.PipeMapRed: PipeMapRed exec [/home/wirarama/hadoop/python/reducer.py]\n",
      "19/09/30 11:03:11 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/09/30 11:03:11 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "19/09/30 11:03:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/09/30 11:03:12 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/09/30 11:03:12 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/09/30 11:03:12 INFO streaming.PipeMapRed: Records R/W=203/1\n",
      "19/09/30 11:03:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "19/09/30 11:03:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "19/09/30 11:03:12 INFO mapred.Task: Task:attempt_local545581902_0001_r_000000_0 is done. And is in the process of committing\n",
      "19/09/30 11:03:12 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/09/30 11:03:12 INFO mapred.Task: Task attempt_local545581902_0001_r_000000_0 is allowed to commit now\n",
      "19/09/30 11:03:12 INFO output.FileOutputCommitter: Saved output of task 'attempt_local545581902_0001_r_000000_0' to hdfs://localhost:9000/user/wirarama/loremipsum-output/_temporary/0/task_local545581902_0001_r_000000\n",
      "19/09/30 11:03:12 INFO mapred.LocalJobRunner: Records R/W=203/1 > reduce\n",
      "19/09/30 11:03:12 INFO mapred.Task: Task 'attempt_local545581902_0001_r_000000_0' done.\n",
      "19/09/30 11:03:12 INFO mapred.Task: Final Counters for attempt_local545581902_0001_r_000000_0: Counters: 29\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=134072\n",
      "\t\tFILE: Number of bytes written=436174\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1391\n",
      "\t\tHDFS: Number of bytes written=1262\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=138\n",
      "\t\tReduce shuffle bytes=2208\n",
      "\t\tReduce input records=203\n",
      "\t\tReduce output records=138\n",
      "\t\tSpilled Records=203\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=242221056\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1262\n",
      "19/09/30 11:03:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local545581902_0001_r_000000_0\n",
      "19/09/30 11:03:12 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "19/09/30 11:03:12 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/09/30 11:03:12 INFO mapreduce.Job: Job job_local545581902_0001 completed successfully\n",
      "19/09/30 11:03:12 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=263696\n",
      "\t\tFILE: Number of bytes written=870140\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2782\n",
      "\t\tHDFS: Number of bytes written=1262\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3\n",
      "\t\tMap output records=203\n",
      "\t\tMap output bytes=1796\n",
      "\t\tMap output materialized bytes=2208\n",
      "\t\tInput split bytes=120\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=138\n",
      "\t\tReduce shuffle bytes=2208\n",
      "\t\tReduce input records=203\n",
      "\t\tReduce output records=138\n",
      "\t\tSpilled Records=406\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=484442112\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1391\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1262\n",
      "19/09/30 11:03:12 INFO streaming.StreamJob: Output directory: /user/wirarama/loremipsum-output\n"
     ]
    }
   ],
   "source": [
    "!~/hadoop/bin/hadoop jar ~/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.7.jar -mapper ~/hadoop/python/mapper.py -reducer ~/hadoop/python/reducer.py -input /user/wirarama/python/loremipsum/* -output /user/wirarama/loremipsum-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aliquam\t1\r\n",
      "Aliquet\t1\r\n",
      "Amet\t1\r\n",
      "Dignissim\t3\r\n",
      "Dolor\t1\r\n",
      "Egestas\t2\r\n",
      "Etiam\t1\r\n",
      "Felis\t1\r\n",
      "Fusce\t1\r\n",
      "In\t1\r\n",
      "Lorem\t1\r\n",
      "Neque\t1\r\n",
      "Nisl\t1\r\n",
      "Nulla\t1\r\n",
      "Nunc\t1\r\n",
      "Pharetra\t2\r\n",
      "Porta\t1\r\n",
      "Sagittis\t1\r\n",
      "Sed\t1\r\n",
      "Sodales\t1\r\n",
      "Tristique\t1\r\n",
      "Volutpat\t2\r\n",
      "Vulputate\t1\r\n",
      "a\t1\r\n",
      "a.\t1\r\n",
      "accumsan.\t1\r\n",
      "adipiscing\t4\r\n",
      "aenean\t1\r\n",
      "aliqua.\t1\r\n",
      "aliquam\t1\r\n",
      "aliquet\t4\r\n",
      "amet\t2\r\n",
      "amet,\t1\r\n",
      "ante\t1\r\n",
      "at\t1\r\n",
      "at.\t1\r\n",
      "bibendum\t2\r\n",
      "blandit\t1\r\n",
      "commodo\t1\r\n",
      "congue\t1\r\n",
      "consectetur\t1\r\n",
      "convallis\t1\r\n",
      "cras\t1\r\n",
      "diam\t3\r\n",
      "diam.\t1\r\n",
      "do\t1\r\n",
      "dolor\t2\r\n",
      "dolor.\t1\r\n",
      "dolore\t1\r\n",
      "donec\t2\r\n",
      "duis.\t1\r\n",
      "egestas\t3\r\n",
      "egestas.\t2\r\n",
      "eget\t4\r\n",
      "eget.\t1\r\n",
      "eiusmod\t1\r\n",
      "eleifend.\t1\r\n",
      "elementum\t1\r\n",
      "elit,\t1\r\n",
      "enim\t2\r\n",
      "enim.\t1\r\n",
      "erat\t1\r\n",
      "est\t1\r\n",
      "et\t5\r\n",
      "etiam\t2\r\n",
      "eu\t1\r\n",
      "facilisis\t1\r\n",
      "faucibus\t2\r\n",
      "felis\t1\r\n",
      "feugiat\t1\r\n",
      "feugiat.\t1\r\n",
      "fringilla\t1\r\n",
      "fringilla.\t1\r\n",
      "gravida.\t1\r\n",
      "in\t1\r\n",
      "in.\t3\r\n",
      "incididunt\t1\r\n",
      "integer\t1\r\n",
      "ipsum\t3\r\n",
      "labore\t1\r\n",
      "lacus\t1\r\n",
      "lectus\t1\r\n",
      "lobortis\t1\r\n",
      "lobortis.\t1\r\n",
      "lorem\t2\r\n",
      "luctus\t2\r\n",
      "maecenas\t1\r\n",
      "magna\t2\r\n",
      "malesuada.\t1\r\n",
      "massa\t1\r\n",
      "mauris.\t1\r\n",
      "mi\t2\r\n",
      "molestie.\t1\r\n",
      "nec\t3\r\n",
      "neque\t1\r\n",
      "netus\t1\r\n",
      "nibh\t3\r\n",
      "nisl\t1\r\n",
      "nulla.\t1\r\n",
      "nunc\t4\r\n",
      "odio\t1\r\n",
      "orci\t4\r\n",
      "pellentesque\t1\r\n",
      "pharetra\t1\r\n",
      "phasellus\t1\r\n",
      "placerat\t1\r\n",
      "porttitor\t1\r\n",
      "praesent\t1\r\n",
      "purus\t1\r\n",
      "quis\t1\r\n",
      "quisque\t1\r\n",
      "risus\t2\r\n",
      "sagittis\t1\r\n",
      "scelerisque\t3\r\n",
      "sed\t5\r\n",
      "semper\t1\r\n",
      "senectus\t1\r\n",
      "sit\t3\r\n",
      "sit.\t1\r\n",
      "sodales\t1\r\n",
      "suscipit.\t1\r\n",
      "suspendisse\t1\r\n",
      "tellus\t1\r\n",
      "tempor\t3\r\n",
      "tortor\t1\r\n",
      "tristique\t1\r\n",
      "tristique.\t1\r\n",
      "turpis\t2\r\n",
      "ullamcorper.\t1\r\n",
      "ultrices\t1\r\n",
      "ut\t3\r\n",
      "vel\t2\r\n",
      "velit\t1\r\n",
      "venenatis\t2\r\n",
      "vitae\t1\r\n",
      "viverra\t2\r\n",
      "viverra.\t1\r\n",
      "volutpat\t1\r\n"
     ]
    }
   ],
   "source": [
    "!~/hadoop/bin/hdfs dfs -cat loremipsum-output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
